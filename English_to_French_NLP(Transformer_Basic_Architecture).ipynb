{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqr_pixn0rso"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/NLP_Projects/NLP_Transformer\")"
      ],
      "metadata": {
        "id": "dKmkQIm00127"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "jVSo-ZUw092l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1:Import Dependences"
      ],
      "metadata": {
        "id": "q6kpUy0U1Hcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "LVz3RZPR1GAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "kTWfo3hH2Eh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Dataset"
      ],
      "metadata": {
        "id": "hTnLjeK-3QEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/NLP_Projects/NLP_Transformer/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "          europarl_en = f.read()\n",
        "with open(\"/content/drive/MyDrive/NLP_Projects/NLP_Transformer/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "          europarl_fr = f.read()\n",
        "with open(\"/content/drive/MyDrive/NLP_Projects/NLP_Transformer/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "          nonbreaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/MyDrive/NLP_Projects/NLP_Transformer/nonbreaking_prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "          nonbreaking_prefix_fr = f.read()"
      ],
      "metadata": {
        "id": "7NFJcCgT2tyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "europarl_fr[:50]"
      ],
      "metadata": {
        "id": "09O4bztXNZ--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "europarl_en[:50]"
      ],
      "metadata": {
        "id": "zK6mFfyY79FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2:Data Preprocessing"
      ],
      "metadata": {
        "id": "iptpcfoUkqhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cleaning Data"
      ],
      "metadata": {
        "id": "zJz4FxqDJeiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the nonbreaking_prefix_en as a clean list of words with a point at the end so it is easier to use."
      ],
      "metadata": {
        "id": "m9o6ezmCKkrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonbreaking_prefix_en = nonbreaking_prefix_en.split(\"\\n\")\n",
        "nonbreaking_prefix_en = [' ' + pref + '.' for pref in nonbreaking_prefix_en]"
      ],
      "metadata": {
        "id": "97e2uMDKDDfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonbreaking_prefix_fr = nonbreaking_prefix_fr.split(\"\\n\")\n",
        "nonbreaking_prefix_fr = [' ' + pref + '.' for pref in nonbreaking_prefix_fr]"
      ],
      "metadata": {
        "id": "PWbK61KXNvzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonbreaking_prefix_en"
      ],
      "metadata": {
        "id": "YUugS7PQJs6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonbreaking_prefix_fr"
      ],
      "metadata": {
        "id": "Uo_YPEAjN4mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will need each word and other symbol that we want to keep to be in a lower case and separeated by spaces so we can \"tokenize\" them."
      ],
      "metadata": {
        "id": "iqxihfzWK6oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_en = europarl_en\n",
        "for prefix in nonbreaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_en)\n",
        "corpus_en = re.sub(r\"\\.###\", '', corpus_en)\n",
        "corpus_en = re.sub(r\" +\", ' ', corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in nonbreaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + \"###\")\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_fr)\n",
        "corpus_fr = re.sub(r\"\\.###\", '',corpus_fr)\n",
        "corpus_fr = re.sub(r\" +\", ' ', corpus_fr)\n",
        "corpus_fr = corpus_fr.split(\"\\n\")"
      ],
      "metadata": {
        "id": "awa6TMSgeiKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizing"
      ],
      "metadata": {
        "id": "ZqvT5LvimPIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "id": "aVaOzDQ1nPGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets"
      ],
      "metadata": {
        "id": "cGKo8SMEnw5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "hSWnlMs2nTD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Create tokenizers\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_fr, target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "1obtGz3Rmbby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "metadata": {
        "id": "O0znVBKPobJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "metadata": {
        "id": "uEQoZuhwpMaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing TOO long sentences"
      ],
      "metadata": {
        "id": "yecFPrkDpc1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we are removing sentences more then 20 length, it will help us to compute faster.If we are having enough ram and GPU than this setp is not for you"
      ],
      "metadata": {
        "id": "lblFFb0BseKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "metadata": {
        "id": "xpwtRumpphfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inputs/Outputs Creation"
      ],
      "metadata": {
        "id": "tEBYrsBPhhzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token,and will make sure this padding token dosen't interfere with our training later."
      ],
      "metadata": {
        "id": "rRSE7lsOhps_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "# Assuming inputs and outputs are sequences to be padded\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding=\"post\",\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding=\"post\",\n",
        "                                                        maxlen=MAX_LENGTH)\n"
      ],
      "metadata": {
        "id": "QQf8jQLkt8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))"
      ],
      "metadata": {
        "id": "cgC6YHwViJZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dateset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vIGz8bjMuLYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stage 3: Model building"
      ],
      "metadata": {
        "id": "Eabmo9pVkODe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embedding"
      ],
      "metadata": {
        "id": "oTPwWUTRkSqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "PE(pos,2i) = sin(pos/100002i/dmodel)\n",
        "\n",
        "PE(pos,2i+1) = cos(pos/100002i/dmodel)"
      ],
      "metadata": {
        "id": "dDsUWEPIkXXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1/np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],  # Check this line\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)\n"
      ],
      "metadata": {
        "id": "vvPF2ObxkSCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention"
      ],
      "metadata": {
        "id": "L0OvoorDrJF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Attention computation"
      ],
      "metadata": {
        "id": "nCxAlDJPrMY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention(Q,K,V)=softmax ((Q * K ^ T)/(sqrt(d_{k}))) * V"
      ],
      "metadata": {
        "id": "C91rjKGUrs2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "    keys_dim = tf.cast(tf.shape[keys][-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "\n",
        "    return attention\n"
      ],
      "metadata": {
        "id": "tQtkA_q6oNXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-head attention sublayer"
      ],
      "metadata": {
        "id": "lLNEBo7MvtKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "    def _init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self)._init_()\n",
        "        self.nb_proj = nb_proj\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model & self.nb_proj == 0\n",
        "\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reashpe(inputs, shape=shape)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_list(keys)\n",
        "        values = self.value_list(values)\n",
        "\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        attention = tf.transpose(attention, param=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1,self.d_model))\n",
        "\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "RsFLQbQbvhbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "fCgkz-I37uAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "         super(EncoderLayer, self).__init__()\n",
        "         self.FFN_units = FFN_units\n",
        "         self.nb_proj = nb_proj\n",
        "         self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayersNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayersNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Z5nVar417e-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def _init_(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"encoder\"):\n",
        "        super(Encoder, self)._init_(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout)\n",
        "                            for _ in range(nb_layers)]\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "expy7pp4f5qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decoder"
      ],
      "metadata": {
        "id": "CdOFotR1nMJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout)\n",
        "                            for _ in range(nb_layers)]\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "wn0TWNyvmbnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout)\n",
        "                            for _ in range(nb_layers)]\n",
        "\n",
        "    def call(self, inputs, enc_outputs,mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Itaxdm2JpcJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer"
      ],
      "metadata": {
        "id": "5p6mw5ephr30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "         super(Transformer, self).__init__(name=name)\n",
        "\n",
        "         self.encoder = Encoder(nb_layers,\n",
        "                                FFN_units,\n",
        "                                nb_proj,\n",
        "                                dropout,\n",
        "                                vocab_size_enc,\n",
        "                                d_model)\n",
        "         self.decoder = Decoder(nb_layers,\n",
        "                                FFN_units,\n",
        "                                nb_proj,\n",
        "                                dropout,\n",
        "                                vocab_size_dec,\n",
        "                                d_model)\n",
        "         self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "\n",
        "    def create_padding_mask(self, seq):  # seq: (batch_size, seq_length)\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "\n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "nQKu_PL6rybh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "6vNsb6mivzfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Hyper Parameters    #Article Hyper Parameters values\n",
        "D_MODEL = 128        #512\n",
        "NB_LAYERS = 4        #6\n",
        "FFN_UNITS = 512      #2048\n",
        "NB_PROJ = 8          #8\n",
        "DROPOUT = 0.1       #0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT,\n",
        "                          name=\"transformer\")"
      ],
      "metadata": {
        "id": "nyXGOPAGv1D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtypes=loss_.dtypes)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "metadata": {
        "id": "pmht7mpPy9YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "         super(CustomSchedule, self).__init__()\n",
        "\n",
        "         self.d_model = tf.cast(d_model, tf.float32)\n",
        "         self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)  # Cast step to float32\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                   beta_1=0.9,\n",
        "                                   beta_2=0.98,\n",
        "                                   epsilon=1e-9)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/NLP_Projects/NLP_Transformer/ckpt\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "metadata": {
        "id": "eUzKN_ps1CMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6x0Xnb52W9_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):  # Use \"in\" instead of \"range\"\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradient(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
        "            ))\n",
        "\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoints for epoch {} at {}\".format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))\n"
      ],
      "metadata": {
        "id": "T2uuXACB4xVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating"
      ],
      "metadata": {
        "id": "eMbeogiRGSww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encoder(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_inputs = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "\n",
        "    for _ in range[MAX_LENGTH]:\n",
        "        predictions = transformer(enc_input, output, False) # (1, seq_length, vocab_size_fr)\n",
        "\n",
        "        prediction = predictions[:, -1:, :]\n",
        "\n",
        "        prediction_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        if prediction_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "\n",
        "        output = tf.concat([output, prediction_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "metadata": {
        "id": "y0bpYmMSF6A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "\n",
        "    predicted_sentence = tokenizer_fr.decoder(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "\n",
        "    print(\"Input: {}.format(sentence)\")\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "metadata": {
        "id": "W9wogVUEZd1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is a problem we have to solve.\")"
      ],
      "metadata": {
        "id": "fOwZiL67aV9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}